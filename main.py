from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sqlalchemy.orm import Session
import numpy as np

from database import Base, engine, get_db
import crud
from schemas import PredictionCreate, PredictionOut

# -------------------------
# INIT DB
# -------------------------
Base.metadata.create_all(bind=engine)
app = FastAPI()

# -------------------------
# CORS (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç)
# -------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
def root():
    return {"msg": "Backend OK - Landmark Model Running"}

# ============================================================
# LOAD MODEL (‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å GitHub Releases)
# ============================================================

import os
import requests
import tflite_runtime.interpreter as tflite

# ‚≠ê ‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å GitHub Releases (binary ‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á)
MODEL_URL = "https://github.com/thinagrit/sign-ai-backend/releases/download/v1.0.0/model.tflite"

# ‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô Render
MODEL_PATH = "/opt/render/project/src/model.tflite"

# üìå ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
if not os.path.exists(MODEL_PATH):
    print("Downloading model from:", MODEL_URL)
    r = requests.get(MODEL_URL)
    with open(MODEL_PATH, "wb") as f:
        f.write(r.content)

# üìå ‡πÇ‡∏´‡∏•‡∏î TFLite Interpreter
interpreter = tflite.Interpreter(model_path=MODEL_PATH)
interpreter.allocate_tensors()

# ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• input/output
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
INPUT_SIZE = input_details[0]["shape"][1]  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô landmark (‡πÄ‡∏ä‡πà‡∏ô 63 ‡∏Ñ‡πà‡∏≤)

# =======================
# LABELS (‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™)
# =======================

LABELS = {
    0: "‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß",     # headache
    1: "‡∏à‡∏≤‡∏°",         # sneeze
}

# ============================================================
# REQUEST MODEL
# ============================================================

class LandmarkInput(BaseModel):
    points: list[float]


@app.post("/translate")
async def translate(payload: LandmarkInput, db: Session = Depends(get_db)):

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô landmark
    if len(payload.points) != INPUT_SIZE:
        return {"error": f"‡∏ï‡πâ‡∏≠‡∏á‡∏™‡πà‡∏á {INPUT_SIZE} ‡∏Ñ‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏™‡πà‡∏á‡∏°‡∏≤ {len(payload.points)}"}

    arr = np.array(payload.points, dtype=np.float32).reshape(1, INPUT_SIZE)

    # ‡∏£‡∏±‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
    interpreter.set_tensor(input_details[0]["index"], arr)
    interpreter.invoke()
    output = interpreter.get_tensor(output_details[0]["index"])

    pred_index = int(np.argmax(output))
    confidence = float(np.max(output))
    label = LABELS.get(pred_index, f"class_{pred_index}")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    saved = crud.create_prediction(
        db,
        PredictionCreate(label=label, confidence=confidence, source="translate")
    )

    return {
        "label": label,
        "confidence": confidence,
        "timestamp": saved.created_at
    }

# ============================================================
# GET DATASET
# ============================================================

@app.get("/dataset", response_model=list[PredictionOut])
def dataset(db: Session = Depends(get_db)):
    return crud.get_all_predictions(db)
